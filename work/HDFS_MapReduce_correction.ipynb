{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initiation à Hadoop et Map-Reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'Objectif du TP: \n",
    "Initiation au framework hadoop et au patron MapReduce, utilisation de docker pour lancer un cluster hadoop.\n",
    "Les consignes de ce notebook sont à réaliser dans votre cluster deployé avec docker, mais vous pouvez utiliser le notebook pour stocker les commandes que vous avez utilisé afin de réaliser une sorte de cheat sheet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Clonez le repo à l’adresse ?? dans votre répertoire de travail.\n",
    "\n",
    "\n",
    "- Démarrez votre image docker: \\\n",
    "Ouvrez votre terminal et placez-vous dans le dossier ?? fraîchement récupéré. \\\n",
    "Lancez la commande: \\\n",
    "\tdocker-compose up -d\n",
    "\n",
    "\n",
    "- Connectez vous au namenode de votre cluster:\\\n",
    "Lancez la commande: \\\n",
    "    docker exec -it namenode bash\n",
    "\n",
    "\n",
    "Refs: \\\n",
    "Apache Hadoop [http://hadoop.apache.org/] \\\n",
    "Docker [https://www.docker.com/]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Placez vous dans le dossier utilisateur: \\\n",
    "cd ~\n",
    "\n",
    "\n",
    "- Créer un répertoire TP, puis deux sous-répertoires code et data dans lesquels sauvegarder respectivement les codes des mappers et reducers, et les données sources et résultats.\n",
    "\n",
    "\n",
    "- En local (en dehors de docker), avec la commande docker adequate envoyez le fichier ventes.txt fourni, dans le répertoire ~/TP/data du namenode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1- Hadoop HDFS\n",
    "   \n",
    "##### Les commandes les plus utilisées dans Hadoop: \n",
    "hadoop fs –ls             Afficher le contenu du répertoire racine\n",
    "\n",
    "hadoop fs –put file.txt   Upload un fichier dans hadoop (à partir du répertoire courant linux)\n",
    "\n",
    "hadoop fs –get file.txt   Download un fichier à partir de hadoop sur votre disque local\n",
    "\n",
    "hadoop fs –tail file.txt  Lire les dernières lignes du fichier\n",
    "\n",
    "hadoop fs –cat file.txt   Affiche tout le contenu du fichier\n",
    "\n",
    "hadoop fs –mv file.txt newfile.txt   Renommer le fichier\n",
    "\n",
    "hadoop fs –rm newfile.txt            Supprimer le fichier\n",
    "\n",
    "hadoop fs –mkdir myinput             Créer un répertoire\n",
    "\n",
    "hadoop fs –cat file.txt | less       Lire le fichier page par page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Créez un répertoire dans HDFS, appelé data.\n",
    "\n",
    "\n",
    "- Envoyez le fichier ventes.txt depuis le dossier ~/TP/data vers le répertoire data HDFS.\n",
    "\n",
    "\n",
    "- Affichez le contenu du répertoire data HDFS et visualisez les dernières lignes du fichier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer le rep data\n",
    "hadoop fs -mkdir /data\n",
    "#si vous avez des soucis de safe mode lancer : sudo -u hdfs hdfs dfsadmin -safemode leave et relancer ensuite mkdir!\n",
    "\n",
    "#Charger le fichier sur HDFS\n",
    "# envoyer le fichier avec docker : docker cp /host/pathto/ventes.txt <containerId>:/container/path/\n",
    "\n",
    "hadoop fs -put ventes.txt /data/\n",
    "hadoop fs -ls /data\n",
    "hadoop fs -tail /data/ventes.txt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-  Map Reduce\n",
    "Un Job Map-Reduce se compose principalement de deux types de programmes:\n",
    "\n",
    "Mappers : permettent d’extraire les données nécessaires sous forme de clef/valeur, pour pouvoir ensuite appliquer le shuffle &sort. \\\n",
    "Reducers : prennent un ensemble de données triées , et effectuent le traitement nécessaire sur ces données (somme, moyenne,total...)\n",
    "\n",
    "PS : Vous aurez besoin d'installer python dans votre namenode docker avec apt-get pour tester vos mapper et reducer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1 )\n",
    "\n",
    "Soit le dataset ventes.txt comportant 6 champs, séparés par des tabulations, de la forme suivante:\n",
    "\n",
    "date temps magasin produit coût paiement\n",
    "\n",
    "Le but est de déterminer le total des ventes par magasin.\n",
    "\n",
    "- Créez un fichier mapper.py dans le dossier ~/TP/code , qui permet de: \\\n",
    "Séparer les différents champs par tabulation. \\\n",
    "Extraire les éléments à partir de ces champs, sous forme de clé/valeur (magasin,coût), Pour calculer les ventes par magasin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mapper.py\n",
    "import sys\n",
    "for line in sys.stdin:\n",
    "    data = line.strip().split('\\t')\n",
    "    if len(data) == 6:\n",
    "        date, time, store, item, cost, payment = data\n",
    "        print ('{0}\\t{1}'.format(store,cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Testez votre mapper sur les 50 premières lignes du fichier ventes.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head -50 ventes.txt | python ./mapper.py echo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2 )\n",
    "Le Reducer permet de faire le traitement définit sur des entrées préalablement triées par Hadoop sur les couples (magasin,coût). \\\n",
    "Le Reducer fera la somme de tous les coûts pour un même magasin. \n",
    "\n",
    "- Créez un fichier reducer.py dans le dossier ~/TP/code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reducer.py\n",
    "import sys\n",
    "salesTotal = 0\n",
    "oldKey = None\n",
    "for line in var:\n",
    "\tdata = line.strip().split('\\t')\n",
    "\tif len(data) != 2:\n",
    "\t\tcontinue\n",
    "\tthisKey, thisSale = data\n",
    "\tif oldKey and oldKey != thisKey:\n",
    "\t\tprint ('{0} {1}'.format(oldKey,salesTotal))\n",
    "\t\toldKey = thisKey;\n",
    "\t\tsalesTotal = 0\n",
    "\toldKey = thisKey\n",
    "\tsalesTotal += float(thisSale)\n",
    "if oldKey != None:\n",
    "    print ('{0} {1}'.format(oldKey, salesTotal))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Testez votre Reducer sur les 50 premières lignes du fichier ventes.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head -50 ventes.txt | python ./mapper.py |sort | python ./reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3)\n",
    "Lancer un job entier sur Hadoop =>  faire appel au mapper puis au reducer sur une entrée volumineuse, et obtenir à la fin un résultat, directement sur HDFS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Exécutez un job hadoop sur le fichier ventes.txt en utilisant les fichiers mapper.py et reducer.py déjà implémentés. \n",
    "- Stocker le résultat dans un répertoire joboutput.\n",
    "\n",
    "\n",
    "- Sauvegarder ensuite le fichier part-00000 dans le dossier ~/TP/data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoop jar /opt/hadoop-3.2.1/share/hadoop/tools/lib/hadoop-streaming-3.2.1.jar --mapper mapper.py --reducer reducer.py --file /root/TP/code/mapper.py --file /root/TP/code/reducer.py --input /data --output /joboutput1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs dfs -get /data/part-00000...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Quelle est la totalité des ventes du magasin de Buffalo ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Créez un nouveau mapper/reducer pour trouver le total des ventes par catégorie de produits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mapper2.py\n",
    "import sys\n",
    "\n",
    "for line in sys.stdin:\n",
    "    data = line.strip().split(\"\\t\")\n",
    "    if len(data) == 6:\n",
    "        date, time, store, item, cost, payment = data\n",
    "print (\"{0}\\t{1}\".format(item, cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Le reducer NE CHANGE PAS!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Exécutez un job hadoop sur le fichier ventes.txt en utilisant les fichiers mapper2.py et reducer.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Quelle est la valeur des ventes pour la catégorie Toys?\n",
    "\n",
    "\n",
    "- Et pour la catégorie Consumer Electronics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bonus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Créez un nouveau mapper/reducer pour faire la liste des montants de ventes pour chaque magasin.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mapper3.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reducer2.py\n",
    "import sys\n",
    "\n",
    "salesMax = 0\n",
    "oldKey = None\n",
    "\n",
    "for line in sys.stdin:\n",
    "    data_mapped = line.strip().split(\"\\t\")\n",
    "    if len(data_mapped) != 2:\n",
    "        # Something has gone wrong. Skip this line.\n",
    "        continue\n",
    "\n",
    "    thisKey, thisSale = data_mapped\n",
    "\n",
    "    if oldKey and oldKey != thisKey:\n",
    "        print oldKey, \"\\t\", salesMax\n",
    "        oldKey = thisKey;\n",
    "        salesMax = 0\n",
    "\n",
    "    oldKey = thisKey\n",
    "    salesMax = max(salesMax, float(thisSale))\n",
    "\n",
    "if oldKey != None:\n",
    "print oldKey, \"\\t\", salesMax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Quelle est la valeur de la vente la plus élévée pour les magasins suivants:\n",
    "\n",
    "o Reno\n",
    "\n",
    "o Toledo\n",
    "\n",
    "o Chandler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Quel est le nombre total des ventes et la valeur totale des ventes de tous magasins confondus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
